<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Machine Learning (Coursera, Andrew Ng) | Kinnara&#39;s Blog 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Kinnara">
    
    

    <meta name="description" content="Some notes I took while taking Machine Learning taught by Andrew Ng on Coursera. BTW, if you also choose to use Python to do the homework assignments rather than Octave&#x2F;Matlab, this tool made by dibge">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning (Coursera, Andrew Ng) | Kinnara&#39;s Blog">
<meta property="og:url" content="http://illithor.github.io/2020/06/23/Machine-Learning-Coursera-Andrew-Ng/index.html">
<meta property="og:site_name" content="Kinnara&#39;s Blog">
<meta property="og:description" content="Some notes I took while taking Machine Learning taught by Andrew Ng on Coursera. BTW, if you also choose to use Python to do the homework assignments rather than Octave&#x2F;Matlab, this tool made by dibge">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week1_Gradient_Descent.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week2_UW_NE.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week2_Normal_Equation.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week2_GD_vs_NE.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week3_One_vs_All.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_Neuron_Model.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_Neural_Network.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_And.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_Or.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_XNOR.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_BP_Alg.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_BP_Alg1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_BP_Alg2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_Gradient_Checking.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_Zero_Init.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Test_Set_Error.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Errors.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Bias_vs_Variance.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Learning_Curves.png">
<meta property="og:image" content="https://www.kdnuggets.com/images/precision-recall-relevant-selected.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_SVM.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_SVMDB.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_LMC_Outlier.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_Sigma.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_Choosing_Landmarks.png">
<meta property="og:image" content="https://stanford.edu/~cpiech/cs221/img/kmeansViz.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_Elbow_Method.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_2D_to_1D.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_3D_to_2D.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_PCA.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_Reconstruction.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Anomaly.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Multivariate_Gaussian_Distribution.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_MGD1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_MGD2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Notations.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Collaborative_Filtering.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Vectorization.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Learning_Curves.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week10_Stochastic_Gradient_Descent.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week10_BGD_vs_SGD.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week11_Text_Detection.png">
<meta property="og:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week11_Character_Segmentation.png">
<meta property="article:published_time" content="2020-06-24T03:09:11.000Z">
<meta property="article:modified_time" content="2023-03-04T02:38:32.247Z">
<meta property="article:author" content="Kinnara">
<meta property="article:tag" content="MOOC">
<meta property="article:tag" content="Data Science">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week1_Gradient_Descent.png">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    
<link rel="stylesheet" href="/css/uno.css">

    
<link rel="stylesheet" href="/css/highlight.css">

    
<link rel="stylesheet" href="/css/archive.css">

    
<link rel="stylesheet" href="/css/china-social-icon.css">


<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        
        <a href="/" title="link to homepage for Kinnara&#39;s Blog"><img src="/avatar.png" width="80" alt="Kinnara&#39;s Blog logo" class="panel-cover__logo logo" /></a>
        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Kinnara&#39;s Blog</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          Fasting, waiting, thinking: three noble and undefeatable feats.
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">Home</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">About</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">Archive</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Machine Learning (Coursera, Andrew Ng)</h1>

    

    <div class="post-meta">
      <time datetime="2020-06-23" class="post-meta__date date">2020-06-23</time> 

      <span class="post-meta__tags tags">

          

          
             &#8226; Tags:
            <font class="tags">
              <a class="tags-link" href="/tags/Data-Science/" rel="tag">Data Science</a>, <a class="tags-link" href="/tags/MOOC/" rel="tag">MOOC</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <p>Some notes I took while taking <a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="noopener">Machine Learning</a> taught by Andrew Ng on Coursera.</p>
<p>BTW, if you also choose to use Python to do the homework assignments rather than Octave/Matlab, <a href="https://github.com/dibgerge/ml-coursera-python-assignments" target="_blank" rel="noopener">this tool</a> made by dibgerge will save your life. You can now get the credit while coding in Python!!!</p>
<h2 id="Week-1-Introduction-Linear-Regression-with-One-Variable-Linear-Algebra-Review"><a href="#Week-1-Introduction-Linear-Regression-with-One-Variable-Linear-Algebra-Review" class="headerlink" title="Week 1 - Introduction, Linear Regression with One Variable, Linear Algebra Review"></a>Week 1 - Introduction, Linear Regression with One Variable, Linear Algebra Review</h2><h3 id="Definitions-of-Machine-Learning"><a href="#Definitions-of-Machine-Learning" class="headerlink" title="Definitions of Machine Learning"></a>Definitions of Machine Learning</h3><p>Arthur Samuel (1959):<br>ML is the field of study that gives computers the ability to learn without being explicitly learned.</p>
<p>Tom Mitchell (1998):<br>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p>
<h3 id="Types-of-Machine-Learning-Algorithms"><a href="#Types-of-Machine-Learning-Algorithms" class="headerlink" title="Types of Machine Learning Algorithms"></a>Types of Machine Learning Algorithms</h3><ul>
<li>Supervised learning:<ul>
<li>teach computers</li>
<li>“right answer” is given</li>
<li>types of problems: regression (continued valued output e.g. housing price), classification (discrete-valued output e.g. malignant or benign tumor)</li>
</ul>
</li>
<li>Unsupervised learning: <ul>
<li>let the computers learn by themselves</li>
<li>no “right answer” is given</li>
<li>a similar example as the tumor example: same points, but instead of blue circles and red crosses, they are not labeled at all</li>
<li>types of problems: clustering (e.g. cluster the two groups of points in the given dataset)</li>
</ul>
</li>
</ul>
<h3 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h3><p>We feed the training set (a group of $(x,y)$’s) to the learning algorithm, and it will produce a hypothesis h (“hypothesis” is the term in ML for a function that does the following mapping). $h$ will map from $x$ to $y$, where $x$ is the known and the $y$ is the result we want (e.g. $x$ is the area of the house and $y$ is the price it worth)</p>
<p>e.g. (univariate) linear regression: $h(x) = \theta_0 + \theta_1 x$</p>
<p>Our goal for linear regression: minimize the cost function (aka square error function)</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m} (h(x_i) - y_i)^2</script><p>where $m$ is the size of the training set.</p>
<p>$h$ is a function of $x$, and $J$ is a function of $\theta_0$ and $\theta_1$.</p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><script type="math/tex; mode=display">
\min_{\theta_0, \theta_1, \ldots, \theta_n} J(\theta_0, \theta_1, \ldots, \theta_n)</script><p>Outline:</p>
<ul>
<li>Start with some $\theta_i$’s</li>
<li>Keep changing their value to reduce the value of the cost function</li>
<li>Imagine: taking one little step from the current point to get as low as possible, and start over again from the new point we stand at - $\theta_0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1)$ and $\theta_1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1)$, where $\alpha$ is the learning rate (the length of each step)</li>
<li>The updates to each $\theta_j$ should be simultaneous</li>
<li>As we approach local minimum, gradient descent will automatically take smaller steps, since the derivatives are closer to 0. And when we finally reach the local minimum, the derivative term will be 0, so we won’t move anymore</li>
</ul>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week1_Gradient_Descent.png" alt="Gradient Descent"></p>
<h2 id="Week-2-Linear-Regression-with-Multiple-Variables"><a href="#Week-2-Linear-Regression-with-Multiple-Variables" class="headerlink" title="Week 2 - Linear Regression with Multiple Variables"></a>Week 2 - Linear Regression with Multiple Variables</h2><h3 id="Multivariate-Linear-Regression"><a href="#Multivariate-Linear-Regression" class="headerlink" title="Multivariate Linear Regression"></a>Multivariate Linear Regression</h3><p>Gradient Descent for multiply variables:</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)</script><p>i.e.</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_j^{(i)}</script><p>for $j \in {0, 1, \ldots, n}$.</p>
<h3 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h3><p>Ideas:</p>
<ol>
<li>Make sure the features are on a similar scale<ul>
<li>e.g. if the contour is of a skewed elliptical shape, gradient descent may take a lot of back and forth (zigzag) steps before reaching the global minimum. However, if we scale the variables, the contour will look much less skewed (like circles in our example), and gradient descent will take less time.</li>
</ul>
</li>
<li>Get every feature into <strong>approximately</strong> a $-1 \leq x_i \leq 1$ range</li>
</ol>
<h3 id="Mean-Normalization"><a href="#Mean-Normalization" class="headerlink" title="Mean Normalization"></a>Mean Normalization</h3><p>Replace $x_i$ with $x_i - \mu_i$ to make features have <strong>approximately</strong> 0 mean (doesn’t apply to $x_0 = 1$)</p>
<p>In a word, replace $x_i$ with $x_i = \frac{x_i - \mu_i}{s_i}$, where $\mu_i$ is the mean of the feature and $s_i$ is the range (max - min) of the feature</p>
<h3 id="Make-sure-gradient-descent-is-working"><a href="#Make-sure-gradient-descent-is-working" class="headerlink" title="Make sure gradient descent is working:"></a>Make sure gradient descent is working:</h3><p>If $J(\theta)$ increases after each iteration, we may consider choosing a smaller learning rate $\alpha$.</p>
<ul>
<li>For sufficiently small $\alpha$, $J(\theta)$ should decrease on every iteration.</li>
<li>But if $\alpha$ is too small, gradient descent can be slow to converge.</li>
</ul>
<h3 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h3><p>Feature scaling is very important!</p>
<p>e.g. size ranges from 1 to 1000, then the square of it will range from 1 to 1,000,000 and so on.</p>
<h3 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h3><p>Instead of using gradient descent and run many iterations, we can also calculate the partial derivations of each $\theta_i$ and let the derivations to be zero. The values of the $\theta_i$’s when the derivations are zero will be the values we want.</p>
<h4 id="How-to-find-the-values-of-the-theta-i-’s"><a href="#How-to-find-the-values-of-the-theta-i-’s" class="headerlink" title="How to find the values of the $\theta_i$’s?"></a>How to find the values of the $\theta_i$’s?</h4><p> (Andrew Ng doesn’t cover this part, so I copied from Linear Algebra 2 Course Notes for MATH 235 by Dan Wolczuk from UWaterloo)</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week2_UW_NE.png" alt="Week2_UW_NE"></p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week2_Normal_Equation.png" alt="Normal Equation"></p>
<p>==If we’re using the normal equation method, then feature scaling isn’t necessary.==</p>
<h4 id="When-to-use-what"><a href="#When-to-use-what" class="headerlink" title="When to use what?"></a>When to use what?</h4><p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week2_GD_vs_NE.png" alt="GD vs NE"></p>
<p>Normal equation works well when $n$ is small (say, &lt; 1000). However, for some more complicated algorithms, normal equation doesn’t work — we still need gradient descent for those algorithms.</p>
<p>But for linear regression, normal equation is a great alternative. Sometimes it can be much faster than gradient descent.</p>
<h3 id="Normal-Equation-Noninvertibility"><a href="#Normal-Equation-Noninvertibility" class="headerlink" title="Normal Equation Noninvertibility"></a>Normal Equation Noninvertibility</h3><p>The reasons why $X^TX$ can be non-invertible could be:</p>
<ul>
<li>redundant features (linearly dependent, e.g. one feature is the size and another feature is size * 2)</li>
<li>too many features (e.g. m &lt;= n)</li>
</ul>
<h2 id="Week-3-Logistic-Regression-Regularization"><a href="#Week-3-Logistic-Regression-Regularization" class="headerlink" title="Week 3 - Logistic Regression, Regularization"></a>Week 3 - Logistic Regression, Regularization</h2><h3 id="Classification-Problems"><a href="#Classification-Problems" class="headerlink" title="Classification Problems"></a>Classification Problems</h3><p>Applying linear regression to a classification problem often isn’t a great idea.</p>
<p>Some reasons (we’re talking about  binary classification problems):</p>
<ul>
<li>recall the example when we add a new point to the RHS and the slope of the line changed a bit, hence changed the prediction for some points.</li>
<li>linear regression can produce values &gt;1 or &lt;0, while we know the results can only be 0 or 1</li>
</ul>
<p>Therefore, let’s take a look at logistic regression where $0 \leq h(x) \leq 1$.</p>
<p>Sigmoid function / logistic function:<br>$h(x) = g(\theta^Tx)$ where $g(z) = \frac{1}{1+e^{-z}}$.<br>That is, $h(x) = \frac{1}{1+e^{-\theta^Tx}}$.</p>
<p>Note that for the sigmoid function, when z &lt; 0, g(z) is close to 0, and when z &gt;= 0, g(z) is close to 1. So according to our definition, if $\theta^Tx &lt; 0$, we say we predict a negative case, while if $\theta^Tx \geq 0$, we predict a positive case.</p>
<p>The boundary we have between the two groups is called the <strong>decision boundary</strong>.</p>
<h3 id="Cost-Function-for-Logistic-Regression"><a href="#Cost-Function-for-Logistic-Regression" class="headerlink" title="Cost Function for Logistic Regression"></a>Cost Function for Logistic Regression</h3><p>Firstly, why can’t we use the same cost function as in linear regression?</p>
<p>That’s because if we use the same</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m} (h(x_i) - y_i)^2</script><p>as in linear regression, the cost function is non-convex (may not end up at a global minimum). We want a convex bowl-shape cost function so that we can end up at the global minimum.</p>
<p>So instead of using the same cost function as in linear regression, we should come up with a new cost function that is convex for logistic regression.</p>
<p>Logistic regression cost function:</p>
<script type="math/tex; mode=display">
\text{Cost}(h_\theta(x), y) = 
\begin{cases}
  -\log(h_\theta(x)) \text{ if } y = 1 \\
  -\log(1 - h_\theta(x)) \text{ if } y = 0\\
\end{cases}</script><p>Write in one line:</p>
<script type="math/tex; mode=display">
\text{Cost}(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))</script><p>So</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{m}\sum_{i=1}^{m} \text{Cost}(h_\theta(x^{(i)}), y^{(i)}) = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))</script><p>Again, by taking derivative:</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)</script><p>That is,</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_j^{(i)}</script><p>This is exactly the same as what we got for linear regression! That because for linear regression, $h(x) = \theta^Tx$ while for logistic regression, $h(x) = \frac{1}{1+e^{-\theta^Tx}}$</p>
<p>We might use a more advanced optimization algorithm (e.g. conjugate gradient/BFGS/L-BFGS etc) because they can be faster and don’t require a learning rate.</p>
<h3 id="Multiclass-classification"><a href="#Multiclass-classification" class="headerlink" title="Multiclass classification"></a>Multiclass classification</h3><p>One-vs-all (one-vs-rest): train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week3_One_vs_All.png" alt="One-vs-All"></p>
<h3 id="The-Problem-of-Overfitting"><a href="#The-Problem-of-Overfitting" class="headerlink" title="The Problem of Overfitting"></a>The Problem of Overfitting</h3><ul>
<li>Underfitting / High bias</li>
<li>“Just right”</li>
<li>Overfitting / High variance</li>
</ul>
<p>The problem of overfitting: if we have too many features, then the model may fit the training set very well. So, the cost function may actually be very close to zero or maybe even zero exactly, but we may then end up with a very wiggly curve (try too hard to fit the training set), so that it may fail to generalize to new examples.</p>
<h3 id="How-to-address-overfitting"><a href="#How-to-address-overfitting" class="headerlink" title="How to address overfitting?"></a>How to address overfitting?</h3><ul>
<li>Reduce the number of features</li>
<li>Regularization</li>
</ul>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>Intuition:</p>
<p>Suppose an overfitting expression is $\theta<em>0 + \theta_1 x + \theta_2 x^2+ \theta_3 x^3 + \theta_4 x^4$. We know it’s a overfitting, and we want $\theta_3$ and $\theta_4$ to be small - we penalize on $\theta_3$ and $\theta_4$:<br>Make the cost function to be $\frac{1}{2m}\sum</em>{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 + 1000\theta_3^2 + 1000\theta_4^2$ and try to minimize this.</p>
<p>This will make $\theta_3$ and $\theta_4$ as close to 0 as possible.</p>
<p>However, if there are many parameters, we don’t which are the ones that should be regularized. In this case, we regularize all the parameters: cost function</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^{n} \theta_j^2]</script><p>where the $\lambda$ is the regularization parameter. It determines how much the costs of our theta parameters are inflated. Also note that for the lambda part, we don’t regularize $\theta_0$.</p>
<h3 id="Gradient-Descent-when-Regularized"><a href="#Gradient-Descent-when-Regularized" class="headerlink" title="Gradient Descent when Regularized"></a>Gradient Descent when Regularized</h3><p>So for gradient descent, the <strong>unregularized</strong> gradient is</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_j^{(i)}</script><p>The <strong>regularized</strong> gradient is</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \theta_i} =
\begin{cases}
   \frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_0^{(i)} \text{ if } j = 0 \\
   \frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j \text{ if } j > 0\\
\end{cases}</script><p>We do the simultaneous update:</p>
<script type="math/tex; mode=display">
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_0^{(i)} \\
\theta_j := \theta_j - \alpha [\frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j] \\
=\theta_j(1 - \alpha\frac{\lambda}{m}) - \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})x_j^{(i)},\;\forall j = 1, 2, \ldots, n</script><h3 id="Normal-Equation-when-Regularized"><a href="#Normal-Equation-when-Regularized" class="headerlink" title="Normal Equation when Regularized"></a>Normal Equation when Regularized</h3><script type="math/tex; mode=display">
\theta = (X^TX + \lambda 
\begin{bmatrix}
0 & 0 & 0 & \ldots & 0\\
0 & 1 & 0 & \ldots & 0\\
0 & 0 & 1 & \ldots & 0\\
0 & 0 & 0 & \ldots & 0\\
0 & 0 & 0 & \ldots & 1\end{bmatrix})^{-1} X^Ty</script><p>The matrix is a $(n+1) * (n+1)$ matrix, and all diagonal entries except for the top left corner is 1.</p>
<h2 id="Week-4-Neural-Networks-Representation"><a href="#Week-4-Neural-Networks-Representation" class="headerlink" title="Week 4 - Neural Networks: Representation"></a>Week 4 - Neural Networks: Representation</h2><h3 id="Non-linear-Hypotheses"><a href="#Non-linear-Hypotheses" class="headerlink" title="Non-linear Hypotheses"></a>Non-linear Hypotheses</h3><p>Why do we need yet another learning algorithm? We already have linear regression and we have logistic regression, so why do we need neural networks?</p>
<p>That’s because when the number of features is large, the computation will be very hard.</p>
<p>For example, if there are $n$ features $x_1, x_2, \ldots, x_n$, and we want to include all the quadratic terms, there would be terms like $x_1^2, x_1x_2, x_1x_3, \ldots, x_2^2, x_2x_3, \ldots$. That’s about $n^2/2$ terms in total.</p>
<p>When $n$  is large, we’ll reach millions of terms.</p>
<h3 id="Model-Representation-1"><a href="#Model-Representation-1" class="headerlink" title="Model Representation"></a>Model Representation</h3><h4 id="Neuron-in-the-brain"><a href="#Neuron-in-the-brain" class="headerlink" title="Neuron in the brain:"></a>Neuron in the brain:</h4><p>Each Neuron has a dendrite (input wire) and an axon (output wire).<br>The process is: get the input from the input wire, do something, pass the result to the output wire.</p>
<p><img alt="Neuron Model" src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_Neuron_Model.png" width="500"></p>
<h4 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network:"></a>Neural Network:</h4><p>An input layer, an output layer, and hidden layer(s).</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_Neural_Network.png" alt="Neural-Network"></p>
<h4 id="Why-Hidden-Layers"><a href="#Why-Hidden-Layers" class="headerlink" title="Why Hidden Layers?"></a>Why Hidden Layers?</h4><p>With the help of the hidden layer, we can learn some pretty interesting and complex features and therefore we can end up with a better hypothesis than if we were constrained to use the raw features $x_1, x_2, x_3$ or if you will constrain to say choose the polynomial terms of $x_1, x_2, x_3$. But instead, this algorithm has the flexibility to try to learn whatever features at once, using these $a_1, a_2, a_3$ in order to feed into this last unit that’s essentially a logistic regression here.</p>
<p>The neural network can use this hidden there to compute more complex features to feed into this final output layer and it can learn more complex hypotheses.</p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><p>And and Or can be computed using two layers:<br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_And.png" alt="And"><br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_Or.png" alt="Or"></p>
<p>If we group up some neural networks we have, we can implement XNOR:<br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week4_XNOR.png" alt="XNOR"></p>
<p>That’s why neural networks can compute pretty complicated functions. When we have multiple layers, we have a relatively simple function of the inputs of the second layer. But the third layer can build on that to complete even more complex functions, and then the layer after that can compute even more complex functions.</p>
<h2 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h2><p>Let’s say that we have a computer vision example, where we’re trying to recognize four categories of objects, and given an image we want to decide if it is a pedestrian, a car, a motorcycle, or a truck.</p>
<p>If that’s the case, what we would do is we would build a neural network with four output units so that our neural network now outputs a vector of four numbers. Then (ideally) $[1, 0, 0, 0]$ will represent a pedestrian, $[0, 1, 0, 0]$ will represent a car, etc.</p>
<h2 id="Week-5-Neural-Networks-Learning"><a href="#Week-5-Neural-Networks-Learning" class="headerlink" title="Week 5 - Neural Networks: Learning"></a>Week 5 - Neural Networks: Learning</h2><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>Recall that the regularized cost function of logistic regression is</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)})) + \frac{\lambda}{2m}\sum_{j=1}^{m}\theta_j^2</script><p>Note that the second summation is from 1 to n, as we don’t regularize the biased term $\theta_0$.</p>
<p>The cost function of the neural network is:</p>
<script type="math/tex; mode=display">
h_\Theta(x) \in R^K \;\;\; (h_\Theta(x))_i = i^{th} \text{output} \\
J(\Theta) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}\log(h_\Theta(x^{(i)}))_k + (1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k) + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{ji}^{(l)})^2</script><p>For the regularization term:<br>The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.</p>
<h3 id="Backpropagation-Algorithm"><a href="#Backpropagation-Algorithm" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h3><ul>
<li>Goal: find the $\Theta$ that minimizes $J(\Theta)$.<br>i.e. find $\min_{\Theta} J(\Theta)$</li>
<li>Intuition: $\delta_j^{(l)}$ is the “error” of node $j$ in layer $l$</li>
</ul>
<p>e.g. if $l = 4$ (we have 4 layers), then</p>
<script type="math/tex; mode=display">
\delta^{(4)} = a^{(4)} - y \\
\delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)} .*g'(z^{(3)}) \;\;\;\text{ where .* is element-wise product}\\
\delta^{(2)} = (\Theta^{(2)})^T\delta^{(3)} .*g'(z^{(2)}) \\
\text{No } \delta^{(1)} \text{ since that is the feature we have and it has no errors}</script><p>Note that when we take the derivative of $g$, we can get $g’(z^{(3)}) = a^{(3)} .* (1 - a^{(3)})$</p>
<p>As for the partial derivative: if we ignore the regularization, then $\frac{\partial{}}{\partial{\Theta_{ij}^{(l)}}}J(\Theta) = a_j^{(l)}d_i^{(l+1)}$</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_BP_Alg.png" alt="BP Alg"></p>
<p>Steps in detail:<br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_BP_Alg1.png" alt="BP Alg1"><br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_BP_Alg2.png" alt="BP Alg2"></p>
<h3 id="Implementation-Note-Unrolling-Parameters-Matrix-gt-Vector"><a href="#Implementation-Note-Unrolling-Parameters-Matrix-gt-Vector" class="headerlink" title="Implementation Note: Unrolling Parameters: Matrix -&gt; Vector"></a>Implementation Note: Unrolling Parameters: Matrix -&gt; Vector</h3><p>The advantages of the matrix representation:</p>
<ul>
<li>more convenient when doing forward propagation and backpropagation</li>
<li>easier for vectorized implementations</li>
</ul>
<p>The advantage of the vector representation:</p>
<ul>
<li>when using the advanced optimization algorithms, those algorithms tend to assume that we have all of our parameters unrolled into a big long vector</li>
</ul>
<h3 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h3><script type="math/tex; mode=display">
\frac{\partial{}}{\partial{\Theta}} J(\Theta) \approx \frac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}</script><p>where $\epsilon$ is a very small number.</p>
<p>For a parameter vector $\theta$, the gradient checking applies as follows:<br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_Gradient_Checking.png" alt="Gradient Checking"></p>
<p>How gradient checking works:</p>
<ol>
<li>First, run backpropagation to get <code>DVec</code></li>
<li>Then, run the numerical gradient algorithm as described above to get <code>gradApprox</code></li>
<li>Check that <code>gradApprox</code> is approximately the same as <code>DVec</code>.</li>
<li>Turn off the numerical gradient algorithm, since it is very slow (compared to backpropagation).</li>
<li>Accept <code>DVec</code> as the result of backpropagation</li>
</ol>
<h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>What should we set the init values of $\Theta$ to be?<br>First, consider all zero ($\Theta<em>{ij}^{(l)} = 0 \;\; \forall i,j, l$). The problem of this is that for a certain $i$ in layer $l$, $\Theta</em>{ij}^{(l)}$ will be the same for all $j$ (see the figure below: the blue arrows will have the same weight, the red arrows will have the same weight, and the green arrows will have the same weight).<br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week5_Zero_Init.png" alt="Zero Init"></p>
<p>Generally speaking, the same will also happen no matter we set $\Theta_{ij}^{(l)} \;\; \forall i,j, l$ to be 0 or some other value — as long as all elements have the same value, it fails to break the symmetry.</p>
<p>The correct way to initialize the values is to set each elements $\Theta_{ij}^{(l)} \;\; \forall i,j, l$ to be a random value in $[-\epsilon, \epsilon]$</p>
<h3 id="Wrap-Up"><a href="#Wrap-Up" class="headerlink" title="Wrap Up"></a>Wrap Up</h3><p>Setting up a neural network:</p>
<ul>
<li>Number of input units: the dimension of features $x^{(i)}$</li>
<li>Number of output units: number of classes</li>
<li>Reasonable default: 1 hidden layer, or if &gt; 1 hidden layer, then each hidden layer has the same number of hidden units</li>
</ul>
<p>Training a neural network:</p>
<ol>
<li>Randomly initialize weights</li>
<li>Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$.</li>
<li>Implement code to comput cost function$J(\Theta)$</li>
<li>Implement back parpagation to compute partial derivatives $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$</li>
<li>Use gradient checking to compare $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$ computed using backpropagation v.s. using numerical estimmate of gradient of $J(\Theta)$.<br>Then disable gradient checking, since it’s very slow.</li>
<li>Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\Theta)$ as a function of parameters $\Theta$</li>
</ol>
<p>The cost function for the neural network is not convex, i.e. we may end up in a local optimum rather than a global optimum. But in most cases, this is not a big deal, because the algorithm will usually find a good enough local optimum.</p>
<h2 id="Week-6-Advice-for-Applying-Machine-Learning-Machine-Learning-System-Design"><a href="#Week-6-Advice-for-Applying-Machine-Learning-Machine-Learning-System-Design" class="headerlink" title="Week 6 - Advice for Applying Machine Learning, Machine Learning System Design"></a>Week 6 - Advice for Applying Machine Learning, Machine Learning System Design</h2><h3 id="Debugging-a-Learning-Algorithm"><a href="#Debugging-a-Learning-Algorithm" class="headerlink" title="Debugging a Learning Algorithm"></a>Debugging a Learning Algorithm</h3><p>If there are errors in the prediction, what should we do?</p>
<ul>
<li>Get more training examples</li>
<li>Try smaller set of features</li>
<li>Try getting additional features</li>
<li>Try adding polynomial features ($x_1^2, x_2^2, x_1x_2$ etc)</li>
<li>Try decreasing $\lambda$</li>
<li>Try increasing $\lambda$</li>
</ul>
<h3 id="The-Test-Set-Error"><a href="#The-Test-Set-Error" class="headerlink" title="The Test Set Error"></a>The Test Set Error</h3><p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Test_Set_Error.png" alt="Test Set Error"></p>
<p>Previously, we divide our set into 70% training set and 30% testing set.</p>
<p>But this has a problem — when we try to find the degree of the polynomial that fits the data, we try from d = 1 to, say, d = 10. We then calculate $J_{test}(\Theta)$ for each different degree, and find the degree d that minimizes the cost.</p>
<p>This is usually an overly optimistic estimate because we choose the model for the test set based on the same test set’s performance.</p>
<p>In order to address this problem, let’s divide the data in this way: 60% training set, 20% cross-validation (CV) set, and 20% test set.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Errors.png" alt="Errors"></p>
<p>So back to the problem to find the best degree d: instead of using the test set to select the degree, we use the cross-validation set to find the d and estimate the error on the test set.</p>
<h3 id="Bias-vs-Variance"><a href="#Bias-vs-Variance" class="headerlink" title="Bias vs Variance"></a>Bias vs Variance</h3><p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Bias_vs_Variance.png" alt="Bias vs Variance"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>x</th>
<th>High bias (underfit)</th>
<th>High variance (overfit)</th>
</tr>
</thead>
<tbody>
<tr>
<td>$J_{train}(\Theta)$</td>
<td>high</td>
<td>low</td>
</tr>
<tr>
<td>$J_{cv}(\Theta)$</td>
<td>high (approx. the same as $J_{train}(\Theta)$)</td>
<td>high ($J<em>{cv}(\Theta) &gt;&gt; J</em>{train}(\Theta)$)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Choosing-the-Regularization-Parameter-lambda"><a href="#Choosing-the-Regularization-Parameter-lambda" class="headerlink" title="Choosing the Regularization Parameter $\lambda$"></a>Choosing the Regularization Parameter $\lambda$</h3><p>Similarly, when we choose the regularization parameter $\lambda$ for a fixed-degreed model, we can also use the training/cv/test method as above. We would try $\lambda = 0, 0.01, 0.02, 0.04, 0.08, \ldots, 10.24$ and find the $\Theta$ that minimized $J_{cv}(\Theta)$ and use that model on our test set.</p>
<h3 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title="Learning Curves"></a>Learning Curves</h3><p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Learning_Curves.png" alt="Learning Curves"></p>
<h3 id="Deciding-What-to-Do"><a href="#Deciding-What-to-Do" class="headerlink" title="Deciding What to Do"></a>Deciding What to Do</h3><p>Back to the examples we discussed at the beginning of this week:</p>
<ul>
<li>Get more training examples —&gt; fixes high variance</li>
<li>Try smaller set of features —&gt; fixes high variance</li>
<li>Try getting additional features —&gt; fixes high bias</li>
<li>Try adding polynomial features ($x_1^2, x_2^2, x_1x_2$ etc) —&gt; fixes high bias</li>
<li>Try decreasing $\lambda$ —&gt; fixes high bias</li>
<li>Try increasing $\lambda$ —&gt; fixes high variance</li>
</ul>
<h3 id="Neural-Network-and-Underfitting-Overfitting"><a href="#Neural-Network-and-Underfitting-Overfitting" class="headerlink" title="Neural Network and Underfitting/Overfitting"></a>Neural Network and Underfitting/Overfitting</h3><ul>
<li>Small neural networks = fewer parameters = more prone to underfitting</li>
<li>Large neural networks = more parameters = more prone to overfitting</li>
<li>The number of hidden layers: using a single hidden layer is a reasonable default, but we can also try to find a training cross-validation and try training neural networks with one hidden layer or more hidden layers, and see which of those neural networks performs best on the cross-validation sets.</li>
</ul>
<h3 id="Machine-Learning-System-Design"><a href="#Machine-Learning-System-Design" class="headerlink" title="Machine Learning System Design"></a>Machine Learning System Design</h3><p>Instead of using gut feeling and starting with a carefully-designed algorithm, try to start with something quick and dirty and test it on cross-validation data quickly.</p>
<p>Plot learning curves and figure out how to improve our algorithm.</p>
<p>It’s usually a better practice to let the error tell us where to spend time for improvement, rather than spending much time on design.</p>
<p>Once the errors are found, manually go through the errors and find specifically a way to improve. (e.g. when building a quick and dirty algorithm for email spam classifier, we found that many “password-stealing” emails are not classified as spams. In this case, we can focus on how to improve the performance of our algorithm on detecting this kind of emails)</p>
<h3 id="Skewed-Classes"><a href="#Skewed-Classes" class="headerlink" title="Skewed Classes"></a>Skewed Classes</h3><p>We have a lot more examples from one class than from the other class. (e.g. 99.5% cancers are benign, 0.5% cancers are malignant)</p>
<p>If we have very skewed classes, it becomes much harder to use just classification accuracy, because we can get very high classification accuracies or very low errors, and it’s not always clear if doing so is really improving the quality of your classifier. (e.g. predicting all cancers to be benign gives us 0.5% error, but it doesn’t seem like a good classifier)</p>
<h3 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h3><p><img src="https://www.kdnuggets.com/images/precision-recall-relevant-selected.jpg" alt="Precision and Recall"></p>
<p>Precision = # of true positives / # of all predicted positives<br>Recall = # of true positives / # of all actual positives</p>
<p>High precision &amp; high recall means that the algorithm is doing well even on skewed classes.</p>
<p>Cancer example:<br>Logistic regression:</p>
<script type="math/tex; mode=display">
\text{Predict} = 
\begin{cases}
  1 \text{ if } y \geq a \\
  0 \text{ if } y < a\\
\end{cases}</script><ul>
<li>Suppose we want to predict y=1 (cancer) only when very confident: increase $a$ - high precision, low recall</li>
<li>Suppose we want to avoid missing too many cases of cancer: decrease $a$ - high recall, low precision.</li>
</ul>
<p>Say we have multiple algorithms that have different precisions and recalls. Which one is the best of them?</p>
<p>Let’s use “F score”: $\frac{2PR}{P+R}$.</p>
<p>High F score = better algorithm.</p>
<p>F score equals 0 if P=0 and R=0. F score equals 1 if P=1 and R=1.</p>
<h2 id="Week-7-Support-Vector-Machines"><a href="#Week-7-Support-Vector-Machines" class="headerlink" title="Week 7 - Support Vector Machines"></a>Week 7 - Support Vector Machines</h2><h3 id="Large-Margin-Classification"><a href="#Large-Margin-Classification" class="headerlink" title="Large Margin Classification"></a>Large Margin Classification</h3><p>Support vector machines:</p>
<p>Recall that the regularized cost function of logistic regression is</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{m}[\sum_{i=1}^{m}y^{(i)}(-\log(h_\theta(x^{(i)}))) + (1-y^{(i)})(-\log(1-h_\theta(x^{(i)})))] + \frac{\lambda}{2m}\sum_{j=1}^{m}\theta_j^2</script><p>and we want to find the $\theta$ such that</p>
<script type="math/tex; mode=display">
\min_{\theta} \frac{1}{m}[\sum_{i=1}^{m}y^{(i)}(-\log(h_\theta(x^{(i)}))) + (1-y^{(i)})(-\log(1-h_\theta(x^{(i)})))] + \frac{\lambda}{2m}\sum_{j=1}^{m}\theta_j^2</script><p>Note that $m$ is a constant in this formula. The value of $m$ doesn’t affect $\theta$.</p>
<p>Let $A = \sum<em>{i=1}^{m}y^{(i)}(-\log(h</em>\theta(x^{(i)}))) + (1-y^{(i)})(-\log(1-h<em>\theta(x^{(i)})))$ and $B=\frac{1}{2}\sum</em>{j=1}^{m}\theta_j^2$. Instead of writing $A+\lambda B$, let’s rewrite it as $CA+B$ (we can see $C$ as $\frac{1}{\lambda}$)</p>
<p>So our formula is:</p>
<script type="math/tex; mode=display">
\min_{\theta} C[\sum_{i=1}^{m}y^{(i)}(-\log(h_\theta(x^{(i)}))) + (1-y^{(i)})(-\log(1-h_\theta(x^{(i)})))]  + \frac{1}{2}\sum_{j=1}^{m}\theta_j^2</script><h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><p>We want $\sum<em>{i=1}^{m}y^{(i)}(-\log(h</em>\theta(x^{(i)}))) + (1-y^{(i)})(-\log(1-h_\theta(x^{(i)})))$ to be as small as possible (close to 0), so we want to meet the requirements below:</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_SVM.png" alt="SVM"></p>
<p>The algorithm will choose the black line as the boundary over the pink line and the green line, as the black line has a larger minimum distance between the positive and negative samples (see the blue bands on both sides of the black line)</p>
<p><img alt="SVMDB" src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_SVMDB.png" width="500"></p>
<p>The value of $C$ decides how fast the boundary will change when there are outliers. For example, in the figure below, the boundary was the black line. If we add an outlier:</p>
<ul>
<li>if $C$ is very large, the new boundary given by the algorithm will be the pink line (but we might not want the boundary to be this sensitive on one outlier)</li>
<li>if $C$ is not very large, the new boundary given by the algorithm will still stay around the black line</li>
</ul>
<p><img alt="LMC Outlier" src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_LMC_Outlier.png" width="500"></p>
<h3 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h3><p>Let’s define $f_1 = \text{similarity}(x, l^{(1)}) = \exp (-\frac{||x - l^{(1)}||^2}{2 \sigma^2})$</p>
<p>What does similarity represent? It represents how close the point $x$ is to the feature we’re talking about (in this case $l^{(i)}$).</p>
<p>If the point $x$ is close to the feature point $l^{(i)}$, then the numerator ($||x - l^{(1)}||^2$) is small, so $f$ is close to 1.</p>
<p>If the point $x$ is far from the feature point $l^{(i)}$, then the numerator ($||x - l^{(1)}||^2$) is large, so $f$ is close to 0.</p>
<p>As for the $\sigma$ in the denominator, it controls how fast the similarity drops from 1 to 0 as the point moves further away from the feature.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_Sigma.png" alt="Sigma"></p>
<h3 id="Choosing-Landmarks"><a href="#Choosing-Landmarks" class="headerlink" title="Choosing Landmarks"></a>Choosing Landmarks</h3><p>How can we choose landmarks?<br>Let the landmarks to be at exactly the same locations as the training samples.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week7_Choosing_Landmarks.png" alt="Choosing Landmarks"></p>
<p>That is, given training samples $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})$, let $l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, \ldots, l^{(m)} = x^{(m)}$.</p>
<p>For training sample $(x^{(i)}, y^{(i)})$, we map $x^{(i)}$ to </p>
<script type="math/tex; mode=display">
f = \begin{bmatrix}
f_1^{(i)}\\
f_2^{(i)}\\
\ldots\\
f_m^{(i)}
\end{bmatrix}</script><p>where</p>
<script type="math/tex; mode=display">
f_1^{(i)} = \text{sim}(x^{(i)}, l^{(1)}), \\
\ldots, \\
f_i^{(i)} = \text{sim}(x^{(i)}, l^{(i)}) = \text{sim}(x^{(i)}, x^{(i)}) = 1, \\
\ldots, \\
f_m^{(i)} = \text{sim}(x^{(i)}, l^{(m)})</script><h3 id="SVM-with-Kernels"><a href="#SVM-with-Kernels" class="headerlink" title="SVM with Kernels"></a>SVM with Kernels</h3><p>We pad $f_0^{(i)} = 1$ to the vector $f$ we have above and get a $m+1$-dimention vector.<br>To train the algorithm:</p>
<script type="math/tex; mode=display">
\min_{\theta} C[\sum_{i=1}^{m}y^{(i)}\text{cost1}(\theta^Tf^{(i)}) + (1-y^{(i)})\text{cost0}(\theta^Tf^{(i)})]  + \frac{1}{2}\sum_{j=1}^{m}\theta_j^2</script><p>See the figure above for the definition of cost0 and cost1.</p>
<h3 id="SVM-Parameters"><a href="#SVM-Parameters" class="headerlink" title="SVM Parameters"></a>SVM Parameters</h3><p>$C(=\frac{1}{\lambda})$:</p>
<ul>
<li>Large C: lower bias, high variance</li>
<li>Small C: higher bias, low variance</li>
</ul>
<p>$\sigma$:</p>
<ul>
<li>Large $\sigma$: feature varies more smoothly. Higher bias, lower variance.</li>
<li>Small $\sigma$: feature varies less smoothly. Lower bias, higher variance.</li>
</ul>
<p>Note:</p>
<p>Do perform feature scaling before using Gaussian Kernel.</p>
<p>If not scaled, e.g. feature 1 is the size of the house (~1000 sqft), and feature 2 is the number of bedrooms (1-5), then the distance $||x-l||^2 = (x_1-l_1)^2+(x_2-l_2)^2+\ldots$ will be dominated by the first feature.</p>
<h3 id="Logistic-Regression-v-s-SVM-when-to-use-which"><a href="#Logistic-Regression-v-s-SVM-when-to-use-which" class="headerlink" title="Logistic Regression v.s. SVM: when to use which?"></a>Logistic Regression v.s. SVM: when to use which?</h3><p>Let n be the number of features, and m be the number of training samples.</p>
<ul>
<li>If n is large (relative to m), use logistic regression, or SVM without a kernel (linear kernel), because we have so many features but a small training set, so a linear function will probably do fine</li>
<li>If n is small and m is intermediate, use SVM with Gaussian kernel</li>
<li>If n is small but m is large, then add more features and use logistic regression, or SVM without a kernel (linear kernel), because SVM with Gaussian will be slow if we have too many training samples</li>
</ul>
<h2 id="Week-8-Unsupervised-Learning-Dimensionality-Reduction"><a href="#Week-8-Unsupervised-Learning-Dimensionality-Reduction" class="headerlink" title="Week 8 - Unsupervised Learning, Dimensionality Reduction"></a>Week 8 - Unsupervised Learning, Dimensionality Reduction</h2><h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><p>Supervised learning:<br>given a labeled training set and the goal is to find the decision boundary that separates the positive label examples and the negative label examples.</p>
<p>Unsupervised learning:<br>given data that does not have any labels associated with it and we just ask the algorithm find some structure in the data for us.</p>
<p>Examples of structures: clusters (clustering algorithm)</p>
<h3 id="K-Means-Algorithm-a-Clustering-Algorithm"><a href="#K-Means-Algorithm-a-Clustering-Algorithm" class="headerlink" title="K-Means Algorithm: a Clustering Algorithm"></a>K-Means Algorithm: a Clustering Algorithm</h3><p><img src="https://stanford.edu/~cpiech/cs221/img/kmeansViz.png" alt="K-Means"></p>
<p>a. List all training examples.<br>b. Init: randomly find two cluster centroids (red and blue)<br>c. Cluster assignment: loop through all training examples. For each of them, assign it to the red centroid if it’s closer to the red centroid, or assign it to the blue centroid if it’s closer to the blue centroid.<br>d. Move centroid: calculate the mean of the red training examples, and move the red centroid to there. Same for blue centroid.<br>e. Repeat step 2 and 3 — new round of color assignment, move centroids, etc.<br>f. After a few rounds, the cluster centroid will not change during the move centroid step.</p>
<p>For the move centroid step: what if no points are assigned to this centroid?<br>In this case, let’s just eliminate the centroid, and our goal will change from “assign the points to K clusters” to “assign the points to K-1 clusters”</p>
<p>If the clusters are non-separated, the K-Means algorithm will still try to cluster them into K clusters.</p>
<h3 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h3><p>The supervised learning algorithms we’ve seen like linear regression and logistic regression have an optimization objective or some cost function that the algorithm was trying to minimize.</p>
<p>It turns out that K-Means also has an optimization objective or a cost function that it’s trying to minimize.</p>
<p>Notations:</p>
<ul>
<li>$c^{(i)}$: index of cluster $(1, 2, \ldots, K)$ to which example $x^{(i)}$ is currently assigned</li>
<li>$\mu_k$: cluster centroid $k$</li>
<li>$\mu_{c^{(i)}}$: cluster centroid of cluster to which example $x^{(i)}$ has been assigned </li>
</ul>
<p>e.g. $x^{(i)}$ is assigned to cluster 5, then $c^{(i)} = 5$, $\mu_{c^{(i)}} = \mu_5$</p>
<p>The cost function (aka distortion) is:</p>
<script type="math/tex; mode=display">
J(c^{(1)}, \ldots, c^{(m)}, \mu _1, \ldots, \mu _K) = \frac{1}{m} \sum_{i=1}^m ||x^{(i)} - \mu _{c^{(i)}}||^2</script><p>Note that $||x^{(i)} - \mu_{c^{(i)}}||^2$ is the distance between the training example and the cluster centroid it’s assigned to.</p>
<p>And the goal is:</p>
<script type="math/tex; mode=display">
\min_{c^{(1)}, \ldots, c^{(m)}, \mu_1, \ldots, \mu_K} J(c^{(1)}, \ldots, c^{(m)}, \mu_1, \ldots, \mu_K)</script><p>Note that it’s not possible for the cost function to increase during each iteration. If you see your cost increasing, there must be something wrong with your code.</p>
<h3 id="Random-Initialization-of-Clusters"><a href="#Random-Initialization-of-Clusters" class="headerlink" title="Random Initialization of Clusters"></a>Random Initialization of Clusters</h3><p>Randomly pick K training examples and set them to be $\mu_1, \ldots, \mu_k$.</p>
<p>If you don’t want to stuck at a local optimum, do multiple retries. For example, do 100 different random initializations and find the best clustering (lowest cost).</p>
<p>If K is small, say less than 10, then multiple random initializations is very helpful to find a good clustering. But if K is large, say 100, then having multiple random initializations is less likely to make a huge difference.</p>
<p>That’s because when K is large, our first random initialization will probably give us a decent solution. All retries after that will provide similar results.</p>
<h3 id="Choosing-the-Number-of-Clusters"><a href="#Choosing-the-Number-of-Clusters" class="headerlink" title="Choosing the Number of Clusters"></a>Choosing the Number of Clusters</h3><ol>
<li>Elbow method (could work, but not usually doing well)<br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_Elbow_Method.png" alt="Elbow Method"></li>
</ol>
<p>But sometimes it’s hard to find the elbow, see the picture on the RHS.</p>
<ol>
<li>Think about the later purpose. How many clusters do we want?<br>e.g. T-shirt size, do we want 3 sizes (S, M, L) or 5 sizes (XS, S, M, L, XL)?</li>
</ol>
<h3 id="Data-Compression"><a href="#Data-Compression" class="headerlink" title="Data Compression"></a>Data Compression</h3><p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_2D_to_1D.png" alt="2D to 1D"><br><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_3D_to_2D.png" alt="3D to 2D"><br>Basic idea: project the points in a N-dimension space to a (N-1)-dimension space.</p>
<h3 id="Principal-Components-Analysis"><a href="#Principal-Components-Analysis" class="headerlink" title="Principal Components Analysis"></a>Principal Components Analysis</h3><p>For the problem of dimensionality reduction, the most commonly used algorithm is principal components analysis, or PCA.</p>
<p>The goal of PCA:</p>
<p><img alt="PCA" src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_PCA.png" width="500"></p>
<p>The length of those blue line segments is called the projection error. What PCA does is to find a lower-dimensional surface onto which to project the data in order to minimize the error.<br>Note that PCA is different from linear regression (least square). For linear regression, the blue line segments are always vertical, while for PCA, the blue line segments are perpendicular to the red line.</p>
<h3 id="PCA-Algorithm-—-reduce-n-dimensional-vectors-to-k-dimensional"><a href="#PCA-Algorithm-—-reduce-n-dimensional-vectors-to-k-dimensional" class="headerlink" title="PCA Algorithm — reduce n-dimensional vectors to k-dimensional"></a>PCA Algorithm — reduce n-dimensional vectors to k-dimensional</h3><ol>
<li>Compute “covariance matrix”<script type="math/tex; mode=display">
\Sigma = \frac{1}{m} \sum_{i=1}^n(x^{(i)})(x^{(i)})^T</script></li>
<li>Computer eigenvectors of matrix $\Sigma$<script type="math/tex; mode=display">
[U, S, V] = svd(sigma) // singular value decomposition</script></li>
<li>What we need from above is the $U$ matrix. Suppose $(x^{(i)})$ is $n \times 1$, then $U$ is a $n \times n$ matrix, and each column of it will be the $u^{(i)}$ vector we want (n vectors in total). To get a k-dimensional space, we can just take the first $k$ columns. Let’s call it $U_{reduce}$ of shape $n \times k$.</li>
<li>Then, for each $x \in R^n$, we want to find a $z \in R^k$ which is a lower-dimensional representation of our vector. Then $z = U_{reduce}^T * x$.</li>
</ol>
<h3 id="Reconstruction-from-the-Compressed-Representation"><a href="#Reconstruction-from-the-Compressed-Representation" class="headerlink" title="Reconstruction from the Compressed Representation"></a>Reconstruction from the Compressed Representation</h3><p>The above algorithm shows how to reduce an n-dimensional vector to a k-dimensional vector. How can we do it backward?</p>
<script type="math/tex; mode=display">
x_{approx}^{(i)} = U_{reduce} * z^{(i)}</script><p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week8_Reconstruction.png" alt="Reconstruction"></p>
<h3 id="How-to-Choose-k-for-PCA"><a href="#How-to-Choose-k-for-PCA" class="headerlink" title="How to Choose k for PCA"></a>How to Choose k for PCA</h3><p>We want to choose a $k$ such that “99% variance is retained”</p>
<script type="math/tex; mode=display">
\frac{\text{average squared projection error}}{\text{total variation}} = 
\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)} - x^{(i)}_{approx}||^2}{\frac{1}{m}\sum_{i=1}^m||x^{(i)} ||^2} \leq 0.01</script><p>In real life, in order to retain 99% of the variance, we can often reduce the dimension of the data significantly and still retain most of the variance.</p>
<p>That’s because, for most real-life data, many features are highly correlated, so it turns out to be possible to compress the data a lot and still retain 99% of the variance.</p>
<p>The algorithm is simply starting from $k = 1$ and check if the result is less than 0.01. If it works, then take the value, else increment $k$ by 1 and repeat.</p>
<p>However, it’s very inefficient.</p>
<p>Instead, let’s use the $S$ in $[U, S, V] = svd(sigma)$.</p>
<script type="math/tex; mode=display">
S = \begin{bmatrix}
S_{11} & 0 & \ldots & 0\\
0 & S_{22} & \ldots & 0\\
0 & 0 & \ddots & \vdots\\
0 & \ldots & 0 & S_{nn}
\end{bmatrix}</script><p>With this, the value above can be easily computed as</p>
<script type="math/tex; mode=display">
\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)} - x^{(i)}_{approx}||^2}{\frac{1}{m}\sum_{i=1}^m||x^{(i)} ||^2} = 1 - \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}</script><p>So we want</p>
<script type="math/tex; mode=display">
\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}} \geq 0.99</script><h3 id="Applying-PCA"><a href="#Applying-PCA" class="headerlink" title="Applying PCA"></a>Applying PCA</h3><p>What PCA does is that it provides a mapping that reduces, say, $x^{(i)} \in R^{10000}$ to $z^{(i)} \in R^{1000}$. The mapping is defined by running PCA <strong>only</strong> on the training set, not the cross-validation set or the testing set.</p>
<p>So the training set will change from $(x^{(i)}, y^{(i)})$ to $(z^{(i)}, y^{(i)}) \;\; \forall i = 1, \ldots, m$. It will speed up our training.</p>
<p>But don’t abuse PCA. Don’t use PCA right away. Always try to train $(x^{(i)}, y^{(i)})$ first — only if it doesn’t work (e.g. very slow, very memory-consuming), then try to use PCA to do the mapping and train $(z^{(i)}, y^{(i)})$.</p>
<h2 id="Week-9-Anomaly-Detection-Recommender-Systems"><a href="#Week-9-Anomaly-Detection-Recommender-Systems" class="headerlink" title="Week 9 - Anomaly Detection, Recommender Systems"></a>Week 9 - Anomaly Detection, Recommender Systems</h2><h3 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h3><p>Given a set of points and a new point $x_{test}$, is it a normal case or an anomaly?</p>
<h3 id="Gaussian-Normal-Distribution"><a href="#Gaussian-Normal-Distribution" class="headerlink" title="Gaussian (Normal) Distribution"></a>Gaussian (Normal) Distribution</h3><script type="math/tex; mode=display">
x \thicksim N(\mu, \sigma^2)</script><p>where $\mu$ is the mean and $\sigma^2$ is the variance.</p>
<script type="math/tex; mode=display">
P(x; \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}\text{exp}(-\frac{(x-\mu)^2}{2\sigma^2})</script><p>When given a data set of $m$ points, how can we estimate a Gaussian distribution for them?</p>
<p>Let $\mu = \frac{1}{m} \sum<em>{i=1}^m x^{(i)}$ and $\sigma^2 = \frac{1}{m} \sum</em>{i=1}^m (x^{(i)} - \mu)^2$</p>
<h3 id="Density-Estimation"><a href="#Density-Estimation" class="headerlink" title="Density Estimation"></a>Density Estimation</h3><p>Let’s assume each feature is distributed according to Gaussian distribution.</p>
<p>That is, given the training set $x^{(1)}, x^{(2)}, \ldots, x^{(m)}$, each $x \in R^n$, we assume</p>
<script type="math/tex; mode=display">
x_1 \thicksim N(\mu_1, \sigma_1^2), \;x_2 \thicksim N(\mu_2, \sigma_2^2), \;\ldots, \;x_n \thicksim N(\mu_n, \sigma_n^2)</script><p>and that</p>
<script type="math/tex; mode=display">
P(x) = P(x_1; \mu_1, \sigma_1^2)P(x_2; \mu_2, \sigma_2^2) \ldots P(x_n; \mu_n, \sigma_n^2) \\
= \prod_{i=1}^n P(x_i; \mu_i, \sigma_i^2)</script><p>When given a new example $x$, calculate $P(x)$ as described above, and report anomaly if $P(x) &lt; \epsilon$.</p>
<p><img alt="Anomaly" src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Anomaly.png" width="500"></p>
<p>The pink shade on the figure on the top matches the pink shade on the figure on the bottom, which labels the area where $P(x) &lt; \epsilon$.</p>
<p>Note that we usually use much more normal training samples than anomalous training samples (e.g. the engine example we use have 10000 normal engines and 20 flawed engines), so we might face a skewed class problem.</p>
<h3 id="Anomaly-Detection-vs-Supervised-Learning"><a href="#Anomaly-Detection-vs-Supervised-Learning" class="headerlink" title="Anomaly Detection vs. Supervised Learning"></a>Anomaly Detection vs. Supervised Learning</h3><p>Why do we use anomaly detection? Since we have a set of labeled data and we want to predict a new data’s label, why don’t we use supervised learning?</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Anomaly Detection</th>
<th>Supervised Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>A very small number of positive examples and a large number of negative examples</td>
<td>Large number of positive and negative examples</td>
</tr>
<tr>
<td>Many different types of anomalies, so it’s hard to “learn” what positive examples look like</td>
<td>Enough positive and negative examples for the algorithm to “get a sense of” what positive examples look like</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Non-Gaussian-Features"><a href="#Non-Gaussian-Features" class="headerlink" title="Non-Gaussian Features"></a>Non-Gaussian Features</h3><p>If some of the features don’t seem to be Gaussian, how can we transform it to be Gaussian?</p>
<p>One way is to use $P(\log(x_i); \mu_i, \sigma_i^2)$ rather than the usual $P(x_i; \mu_i, \sigma_i^2)$ form.</p>
<p>Some other examples are: $P(\log(x_i+1); \mu_i, \sigma_i^2)$, $P(\sqrt{x_i}; \mu_i, \sigma_i^2)$, $P(x_i^{0.75}; \mu_i, \sigma_i^2)$ etc. Just play around with it and make the data look more Gaussian.</p>
<h3 id="Multivariate-Gaussian-Distribution"><a href="#Multivariate-Gaussian-Distribution" class="headerlink" title="Multivariate Gaussian Distribution"></a>Multivariate Gaussian Distribution</h3><p>Sometimes, anomaly detection fails to recognize an anomaly, e.g. the green dot below. The algorithm has seen even less $x_1$ and even larger $x_2$, so it thinks the green dot is also normal.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Multivariate_Gaussian_Distribution.png" alt="Multivariate Gaussian Distribution"></p>
<p>Multivariate Gaussian Distribution: Instead of modelling $P(x_1),P(x_2)$ separately, model $P(x)$ in one go — $P(x; \mu, \sigma)$.</p>
<p>Parameters: $\mu \in R^n, \; \Sigma \in R^{n \times n}$</p>
<p>Some examples:</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_MGD1.png" alt="MGD1"></p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_MGD2.png" alt="MGD2"></p>
<h3 id="Multivariate-Gaussian-Distribution-v-s-the-Original-Model"><a href="#Multivariate-Gaussian-Distribution-v-s-the-Original-Model" class="headerlink" title="Multivariate Gaussian Distribution v.s. the Original Model"></a>Multivariate Gaussian Distribution v.s. the Original Model</h3><p>Turns out that for the original models, the contours of the graph are always axis aligned, i.e. their $\Sigma$ matrix’s non-diagonal entries are all 0, so the features’ covariance is 0.</p>
<p>With the original models, we can’t have contours that look like the 45 degrees tilted version as we have in the figure above.</p>
<p>The original version cannot capture the correlations between features, so sometimes we need to add some other features to make it work, e.g. apart from <code>CPU_COUNT</code> and <code>NETWORK_SPEED</code>, we may also need to add a feature <code>CPU_COUNT^2 / NETWORK_SPEED</code>.</p>
<p>Multivariate Gaussian Distribution, on the other hand, can capture the correlations between features automatically, so we don’t need to add new features.</p>
<p>But Multivariate Gaussian Distribution is computationally more expensive. So most of the time people use the original model plus manually design some new features.</p>
<h3 id="Recommander-Systems"><a href="#Recommander-Systems" class="headerlink" title="Recommander Systems"></a>Recommander Systems</h3><p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Notations.png" alt="Notations"></p>
<h3 id="Content-Based-Recommendations"><a href="#Content-Based-Recommendations" class="headerlink" title="Content Based Recommendations"></a>Content Based Recommendations</h3><p>Optimization objective:</p>
<p>To learn $\theta^{(j)}$ (parameter for user j)</p>
<script type="math/tex; mode=display">
\min_{\theta^{(j)}} \frac{1}{2} \sum_{i:r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum_{k = 1}^n(\theta_k^{(j)})^2</script><p>To learn $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}$ (parameter for all users):</p>
<script type="math/tex; mode=display">
\min_{\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}} \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}  \sum_{j=1}^{n_u} \sum_{k = 1}^n(\theta_k^{(j)})^2</script><p>Note that originally the coefficients are $\frac{1}{2m^{(j)}}$ and $\frac{\lambda}{2m^{(j)}}$, but since $m^{(j)}$ is a constant, we can ignore it for both denumerator.</p>
<p>Gradient descent:</p>
<script type="math/tex; mode=display">
\theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)}) x_k^{(i)} \;\; \text{when } k = 0 \\
\theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i, j) = 1} (((\theta^{(j)})^Tx^{(i)} - y^{(i, j)}) x_k^{(i)} + \lambda \theta_k^{(j)}) \;\; \text{when } k \neq 0</script><p>What if the movies are not content-based?</p>
<h3 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h3><p>The algorithm has a very interesting property — feature learning. It means that the algorithm can start to learn for itself what features to use.</p>
<p>Let’s say we don’t know the features $x$ for the movies, but we do know the users’ ratings and their $\theta$’s.</p>
<p>From the four equations in the bottom right corner, we can roughly guess that $x^{(1)} = [1, 1.0, 0.0]^T$.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Collaborative_Filtering.png" alt="Collaborative Filtering"></p>
<p>So the optimization algorithm will be given $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}$ to learn $x^{(i)}$:</p>
<script type="math/tex; mode=display">
\min_{x^{(i)}} \frac{1}{2} \sum_{j:r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2} \sum_{k = 1}^n(\theta_k^{(i)})^2</script><p>Similarly, given $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}$ to learn $x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}$:</p>
<script type="math/tex; mode=display">
\min_{x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}  \sum_{i=1}^{n_m} \sum_{k = 1}^n(\theta_k^{(i)})^2</script><div class="table-container">
<table>
<thead>
<tr>
<th>Content Based</th>
<th>Collaborative Filtering</th>
</tr>
</thead>
<tbody>
<tr>
<td>Given features $x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}$ and user ratings, we can estimate $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}$</td>
<td>Given $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}$, we can estimate features $x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}$.</td>
</tr>
</tbody>
</table>
</div>
<p>This is like a chick and egg problem. In reality, we can do $\theta \rightarrow x \rightarrow \theta \rightarrow x \rightarrow \ldots$ to keep estimating and converge to a reasonable result.</p>
<p>However, there is also an efficient algorithm to not go back and forth but do the same thing, which is to put them together.</p>
<p>Our new cost function:</p>
<script type="math/tex; mode=display">
\min_{x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}, \theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}} \frac{1}{2} \sum_{(i, j):r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 + \frac{\lambda}{2}  \sum_{i=1}^{n_m} \sum_{k = 1}^n(\theta_k^{(i)})^2 + \frac{\lambda}{2}  \sum_{j=1}^{n_u} \sum_{k = 1}^n(\theta_k^{(j)})^2</script><p>Algorithm:</p>
<ol>
<li>Init $x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}, \theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}$ to small random values</li>
<li><p>Minimize the cost function above using gradient descent</p>
<script type="math/tex; mode=display">
x_k^{(i)} := x_k^{(i)} - \alpha \sum_{j:r(i, j) = 1} (((\theta^{(j)})^Tx^{(i)} - y^{(i, j)}) \theta_k^{(j)} + \lambda x_k^{(i)}) \\
\theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i, j) = 1} (((\theta^{(j)})^Tx^{(i)} - y^{(i, j)}) x_k^{(i)} + \lambda \theta_k^{(j)})</script><p>Note that the special case for 0 is gone since if we combine the two to the new cost function as above, we eliminate the limit that $x_0 = 1$ and $\theta_0 = 1$. That is because if the algorithm really needs a term to be 0 all the time, it can train a term like this itself.</p>
</li>
<li><p>Our predicting result will be $\theta^Tx$</p>
</li>
</ol>
<h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>The matrix in the top right corner equals to$X\Theta^T$.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week9_Vectorization.png" alt="Vectorization"></p>
<h3 id="Application-Similar-Movies"><a href="#Application-Similar-Movies" class="headerlink" title="Application: Similar Movies"></a>Application: Similar Movies</h3><p>Let’s say movie $i$’s features is $x^{(i)}$. How can we find a movie $j$ that is similar to the movie $i$?</p>
<p>We can do so by finding a movie with features $x^{(j)}$ such that $||x^{(i)} - x^{(j)}||$ is small.</p>
<p>What if there is a user that hasn’t given any rating to any movie?</p>
<p>If we go to the cost function above, the first term will be gone since none of $r(i, j) = 1$ for that user (they didn’t give any rating). So what we try to minimize will be $\frac{\lambda}{2}  \sum<em>{j=1}^{n_u} \sum</em>{k = 1}^n(\theta_k^{(j)})^2$, which will give us $\theta = 0$.</p>
<p>But it doesn’t make sense to say that this user dislikes all movies.</p>
<p>So rather than giving all zeros, let’s assume the user’s rating for a movie is the average rating of that movie from other users who rated that movie.</p>
<h2 id="Week-10-Large-Scale-Machine-Learning"><a href="#Week-10-Large-Scale-Machine-Learning" class="headerlink" title="Week 10 - Large Scale Machine Learning"></a>Week 10 - Large Scale Machine Learning</h2><p>“In machine learning, it’s not who has the best algorithm that wins. It’s who has the most data.”</p>
<p>A problem with learning with large datasets is that recall for gradient descent, we have a summation from 1 to m where m is the size of the dataset. So for each gradient descent, we’ll need to run the summation over the entire dataset, which is very inefficient.</p>
<p>How can we know if a dataset is big enough? Or should we add more data to it to get a better training result?</p>
<p>The answer is: learning curves! (week 6)</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week6_Learning_Curves.png" alt="Learning Curves"></p>
<p>Plot a learning curve and see whether it’s a high variance or a high bias. If high variance, then increasing the data size will give us a better result, while if it’s a high bias, increasing the data size won’t help.</p>
<h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>As stated above, when the data size is large, our old gradient descent (batch gradient descent) will be very expensive to calculate. To scale the algorithm for larger datasets, we’ll use stochastic gradient descent.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week10_Stochastic_Gradient_Descent.png" alt="Stochastic Gradient Descent"></p>
<p>Note that we no longer have that summation in the equation. We improve each $(x^{(i)}, y^{(i)})$ only on its old value, rather than waiting for the summation to get all the old values.</p>
<p>In the figure below:</p>
<ul>
<li>red is for batch gradient descent: always moves to a better value</li>
<li>pink is for stochastic gradient descent: takes more steps since each iteration is faster, but not every step is a good direction</li>
</ul>
<p><img alt="BGD vs SGD" src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week10_BGD_vs_SGD.png" width="500"></p>
<h3 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h3><ul>
<li>Batch gradient descent: use all $m$ examples in each iteration</li>
<li>Stochastic gradient descent: use 1 examples in each iteration</li>
<li>Mini-batch gradient descent: use $b$ examples in each iteration ($b$ is up to our choice, usually from 2 to 100)</li>
</ul>
<p>e.g. when $b = 10$:</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j - \alpha \frac{1}{10}\sum_{i=k}^{k+9} (h(x^{(i)}) - y^{(i)})x_j^{(i)}</script><p>And in each iteration, increase $k$ by 10.</p>
<h3 id="Checking-for-Convergence"><a href="#Checking-for-Convergence" class="headerlink" title="Checking for Convergence"></a>Checking for Convergence</h3><p>When doing batch gradient descent, we can make sure that the function converges by looking at the cost function — it should be non-increasing.</p>
<p>How can we check for stochastic gradient descent?<br>The answer is to compute $cost(\theta, (x^{(i)}, y^{(i)}))$ before updating $\theta$ using $(x^{(i)}, y^{(i)})$. And for every, say, 1000 iterations, plot the average cost of the last 1000 iterations and see how well the algorithm runs.</p>
<p>Usually, stochastic gradient descent ends up in a place that is close to the global minimus (but not exactly the global minimum). If we want stochastic gradient descent to actually converge to the global minimum, we can slowly <strong>decrease</strong> the learning rate alpha over time: $\alpha = \frac{\text{const1}}{\text{iterationNumber + const2}}$.</p>
<p>But people usually don’t do this, because it requires some tuning on the constants. But if we can tune it well, we will reach the global minimum.</p>
<h3 id="Online-Learning"><a href="#Online-Learning" class="headerlink" title="Online Learning"></a>Online Learning</h3><p>Let this code run forever on the server:</p>
<script type="math/tex; mode=display">
\text{get } (x, y )\text{ from the user and update } \theta \text{ using } (x, y):\\
\theta_j := \theta_j - \alpha (h(x) - y)x_j \;\;\forall j = 1, \ldots, n\\
\text{after that, we can discard } (x, y)</script><p>If many users keep arriving at the website, we’ll have enough data for training. And this is very adaptive e.g. when the trend changes, $\theta$ can correspond to the change because of tons of updated data from the users.</p>
<h2 id="Week-11-Photo-OCR"><a href="#Week-11-Photo-OCR" class="headerlink" title="Week 11 - Photo OCR"></a>Week 11 - Photo OCR</h2><h3 id="Photo-OCR-Pipeline"><a href="#Photo-OCR-Pipeline" class="headerlink" title="Photo OCR Pipeline"></a>Photo OCR Pipeline</h3><ol>
<li>Text detection: find the text regions on the photo</li>
<li>Character segmentation: segment the text into individual chars</li>
<li>Character classification</li>
</ol>
<h3 id="Sliding-Window"><a href="#Sliding-Window" class="headerlink" title="Sliding Window"></a>Sliding Window</h3><p>e.g. pedestrian detection</p>
<ol>
<li>Train some sample patches of size $w * h$</li>
<li>Slide a frame of size $w * h$ through the photo, see if the segment in the frame is positive/negative</li>
<li>Slide the frame by <code>step size</code> (the smaller the better, but also more computationally expensive)</li>
<li>Resize the frame to a bigger/smaller size, repeat the steps above (because the size of pedestrians in the photo are different depending on how close/far they are)</li>
</ol>
<h3 id="Text-Detection"><a href="#Text-Detection" class="headerlink" title="Text Detection"></a>Text Detection</h3><p>We’ll run the sliding window algorithm and find the characters, then “expand” the text region.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week11_Text_Detection.png" alt="Text Detection"></p>
<p>How to know where to split the character?</p>
<p>Look for a blank line in the middle between two chars.</p>
<p><img src="https://raw.githubusercontent.com/illithor/Images/master/Andrew_Ng_ML/Week11_Character_Segmentation.png" alt="Character Segmentation"></p>
<h3 id="Artificial-Data-Synthesis"><a href="#Artificial-Data-Synthesis" class="headerlink" title="Artificial Data Synthesis"></a>Artificial Data Synthesis</h3><p>If the number of training data is not enough, we can synthesize data. But before we do that, let’s consider the following:</p>
<ol>
<li>Make sure we have a low bias classifier before synthesizing more data. If the classifier is of high bias, increasing the number of training data wouldn’t help.</li>
<li>How much work will it take to synthesize 10x more data?</li>
</ol>
<h3 id="What-Part-of-the-Pipeline-to-Work-on-Next？"><a href="#What-Part-of-the-Pipeline-to-Work-on-Next？" class="headerlink" title="What Part of the Pipeline to Work on Next？"></a>What Part of the Pipeline to Work on Next？</h3><p>Use Celling Analysis.</p>
<p>Let’s say we have a pipeline:</p>
<p>Step 1 —-&gt; Step 2 —-&gt; Step 3</p>
<ol>
<li>Check the overall accuracy of the pipeline: 72%</li>
<li>Instead of using the output from <code>Step 1</code>, manually give <code>Step 2</code> 100% accurate data as input, and the accuracy of the entire pipeline is now 89%</li>
<li>Instead of using the output from <code>Step 2</code>, manually give <code>Step 3</code> 100% accurate data as input, and the accuracy of the entire pipeline is now 99%</li>
</ol>
<p>So we know that: if we improve <code>Step 1</code>‘s accuracy, the overall accuracy goes up by 17%, and if we improve <code>Step 2</code>‘s accuracy, the overall accuracy goes up by 10%, etc.</p>
<p>Therefore, we should put more effort into <code>Step 1</code>.</p>

  </section>

  

<section class="post-comments">

    <!-- 将评论系统（例如Disqus、多说、友言、畅言等）提供的代码片段粘贴在这里 -->
    <div id="disqus_thread"></div>
    <script>
    var disqus_config = function () {
        this.page.url = 'http://illithor.github.io/2020/06/23/Machine-Learning-Coursera-Andrew-Ng/';
        this.page.identifier = '/2020/06/23/Machine-Learning-Coursera-Andrew-Ng/';
    };
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://shirak-me.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
</section>





</article>


            <footer class="footer">

    
    <script type="text/javascript">
    var disqus_shortname = 'shirak-me';
    var disqus_config = function () {
        this.page.url = 'http://illithor.github.io/2020/06/23/Machine-Learning-Coursera-Andrew-Ng/';
        this.page.identifier = '/2020/06/23/Machine-Learning-Coursera-Andrew-Ng/';
        this.page.title = 'Machine Learning (Coursera, Andrew Ng)';
    };
    (function(){
      var d = document;
      var dsq = d.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (d.head || d.body).appendChild(dsq);
    })();
    </script>
    

    <span class="footer__copyright">&copy; 2014-2025. | Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> | Theme <a href="https://github.com/someus/huno" target="_blank" rel="noopener">Huno</a> | Made with ♥ by <a href="/about">Kinnara</a></span>
    
</footer>

        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
      

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    
    

    <script src="/js/jquery.min.js"></script>
    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
